{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden der Daten für weitere Analysen\n",
    "\n",
    "Wir laden die Daten.\n",
    "\n",
    "Wählen Items aus.\n",
    "\n",
    "Und -- in Zukunft -- säubern (z.B. missing values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotheken, die wir brauchen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'germany'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15536/800633091.py:7: DtypeWarning: Columns (76,428,451,453,455,457,459,464,495,505) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfo = pd.read_csv(datapath+filename+ext, usecols=allcols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data loaded. It is of size: (54505, 579)\n",
      "Adjust times done.\n",
      "\n",
      "Cell Completed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datapath = '../../data/'\n",
    "filename = f'all_countries_numerical_'+country\n",
    "ext = '.csv'\n",
    "\n",
    "allcols=['recordno','endtime','qweek','i1_health','i2_health','i7a_health','i3_health','i4_health','i5_health_1','i5_health_2','i5_health_3','i5_health_4','i5_health_5','i5_health_99','i5a_health','i6_health','i7b_health','i8_health','i9_health','i10_health','i11_health','i12_health_1','i12_health_2','i12_health_3','i12_health_4','i12_health_5','i12_health_6','i12_health_7','i12_health_8','i12_health_9','i12_health_10','i12_health_11','i12_health_12','i12_health_13','i12_health_14','i12_health_15','i12_health_16','i12_health_17','i12_health_18','i12_health_19','i12_health_20','i13_health','i14_health_1','i14_health_2','i14_health_3','i14_health_4','i14_health_5','i14_health_6','i14_health_7','i14_health_8','i14_health_9','i14_health_10','i14_health_96','i14_health_98','i14_health_99','i14_health_other','d1_health_1','d1_health_2','d1_health_3','d1_health_4','d1_health_5','d1_health_6','d1_health_7','d1_health_8','d1_health_9','d1_health_10','d1_health_11','d1_health_12','d1_health_13','d1_health_98','d1_health_99','weight','age','gender','state','household_size','employment_status','wcrex2','wcrv_4','core_b2_4','cantril_ladder','phq4_1','phq4_2','phq4_3','phq4_4','m1_1','m1_2','m1_3','m1_4','m2','m3','m4_1','m4_2','m4_3','m4_4','m4_96','m4_99','m4_other','m5_1','m5_2','m6_1','m6_2','m6_3','m6_4','m6_5','m6_6','m6_7','m6_8','m6_96','m6_other','m7_1','m7_2','m7_3','m7_4','m7_5','m7_6','m7_8','m7_9','m7_10','m7_11','m8_1','m8_2','m8_3','m8_4','m8_5','m8_6','m8_7','m8_8','m8_96','m8_99','m8_other','m9_1','m9_2','m9_3','m9_4','m9_5','m9_6','m9_7','m10','m11','m12_1','m12_2','m12_3','m12_4','m12_5','m12_6','m12_7','m12_8','m12_96','m12_99','m12_other','m13_1','m13_2','m13_3','m13_4','m13_5','m13_6','m13_8','m13_9','m13_10','m13_11','v1','v2_1','v2_2','v2_3','v2_4','v2_5','v2_99','v3','v3_open','v4_1','v4_2','v4_3','v4_4','v4_5','v4_96','v4_99','v3_other','wcrex1','i12_health_21','i12_health_22','i12_health_23','i12_health_24','i12_health_25','r1_1','r1_2','r1_3','r1_4','r1_5','r1_6','r1_7','m7_other','m14_1','m14_2','m14_3','m14_4','m14_5','m14_6','m14_7','m14_8','m14_9','m14_10','m14_11','m14_96','m14_99','m14_open','i14_health_11','ct1_1','ct1_2','ct1b_1','ct1b_2','ct1b_3','ct1b_4','ct1b_5','ct1b_6','ct1b_7','ct1b_8','ct1b_9','ct1b_10','ct1b_11','ct1b_96','ct1b_99','ct1b_other','ct2','ct3','ct3_other','ct4','ct5_1','ct5_2','ct5_3','ct5_4','ct5_5','ct5_6','ct5_7','ct5_8','ct5_96','ct5_99','ct5_other','ct6_1','ct6_2','ct6_3','ct6_4','ct6_5','ct6_6','ct6_96','ct6_99','ct6_other','ct7_1','ct7_2','ct7_3','ct8_1','ct8_2','ct8_3','ct8_4','ct8_5','ct8_6','ct8_99','i12_health_26','ox1_1','ox1_2','ox3_1','ox3_2','ox3_3','ox3_4','ox3_5','ox3_6','ox5_1','ox5_2','ox6','ox4_1_1','ox4_2_1','ox4_3_1','ox4_4_1','ox4_5_1','ox4_6_1','ox4_7_1','ox4_8_1','ox4_9_1','ox4_10_1','ox4_11_1','w1','w2','w3','w4_1','w4_2','w4_3','w4_4','w4_5','w4_6','w4_7','w4_8','w4_9','w4_10','w4_11','w4_99','w4b','w5_1','w5_2','w5_3','w5_4','w5_5','w5_6','w5_7','w5_8','w5_9','w5_10','w5_11','w5_12','w5_13','w5_14','w5_15','w5_16','w5_96','w5_99','w6','w7','w9_1','w9_2','w9_3','w9_4','w9_5','disability','work1','work2','work3','work4','work5','work6','work7_1','work7_2','work7_3','work7_4','work7_5','work8','i12_health_27','i12_health_28','i12_health_29','soc1_1','soc1_2','soc1_3','soc2_1','soc2_2','soc2_3','soc2_4','soc2_5','soc2_6','soc2_open','vac_1','vac_2','vac2_1','vac2_2','vac2_3','vac2_4','vac2_5','vac2_6','vac_3','vac3_1','vac3_2','vac3_3','vac3_4','vac3_5','vac3_6','vac3_7','vac3_8','vac3_9','vac3_other','vac4','vac5','vac6','vac7','vac8','av1_1','av1_2','av1_3','av1_99','av2','r1_8','r1_9','r1_10','vac2_7','av3','vac','vac9','vac10_1','vac10_2','vac10_3','vac10_4','vac10_5','vac10_99','vac11','travel1_1','travel1_2','travel1_3','travel1_4','travel1_99','vac12_1','vac12_2','vac12_3','vac12_4','vac12_11','vac12_5','vac12_6','vac12_7','vac12_8','vac12_9','vac12_10','vac12_96','vac12_other','vac13','vac10_8','vac_booster','vac12_12','vac12_13','vac14_1','vac14_2','vac14_3','child_age_1','child_age_2','child_age_3','child_age_4','child_age_5','child_education_1','child_education_2','child_education_3','child_education_4','childvac','combi_vac','household_children','vent_1','vent_2','vent_3','wah_1','wah_2','wah_3','wah4','wah5','wah7_1','wah7_2','wah7_3','wah7_4','wah7_5','wah7_6','wah7_7','wah7_99','wah6','v1_1','v1_2','v1_3','v1_98','v3_me','v3_me_other','v3_baby','v3_baby_other','v3_child2_4','v3_child2_4_other','v3_child5_17','v3_child5_17_other','v3_adult18','v3_adult18_other','v4_8','v4_9','v4_6','v4_98','v4_other','sc_1_1','sc_1_2','sc_1_3','sc_1_4','sc_1_5','sc_1_6','sc_1_7','sc_1_99','sc_2','sc_3_1','sc_3_2','sc_3_3','sc_3_4','sc_3_5','sc_3_6','sc_3_7','sc_3_99','vac_boost_1','vac_boost_2','vac12_booster_1','vac12_booster_2','vac12_booster_3','vac12_booster_4','vac12_booster_5','vac12_booster_6','vac12_booster_7','vac12_booster_8','vac12_booster_9','vac12_booster_10','vac12_booster_96','vac12_booster_other','vac_man_1','vac_man_2','vac_man_3','vac_man_4','vac_man_5','vac_man_6','vac_man_7','vac_man_96','vac_man_99','q_other','household_children_resp','had_covid','vac_boost_beyond','future_1','future_2','had_covid_2','long_covid','future_3','country','case_id','region','employment_status_1','employment_status_2','employment_status_3','employment_status_4','employment_status_5','employment_status_6','employment_status_7','profile_work_stat','i5_health_98','ct5_9','disability_eu','w4_98','w5_98','record','house_rooms','sw_1_1','sw_1_2','sw_1_3','sw_1_4','sw_1_5','sw_1_6','sw_1_7','sw_1_8','sw_1_9','sw_1_10','sw_1_11','sw_1_12','sw_1_13','sw_1_14','sw_1_15','sw_1_98','sw_1_other','sw_2','s1_1','s1_2','s1_3','s1_4','s1_5','s1_6','s1_7','s1_8','s1_9','s1_10','s1_11','s1_12','s1_13','s1_14','s1_17','s1_15','s1_16','inputstate','profile_household_children','city','region_fi','house1','house2','q3_1','q3_2','q3_3','q3_4','i12_health_21_ph','i12_health_22_ph','i12_health_23_ph']\n",
    "\n",
    "dfo = pd.read_csv(datapath+filename+ext, usecols=allcols)\n",
    "\n",
    "print('Original data loaded. It is of size:',dfo.shape)\n",
    "\n",
    "dfo['endtime']=pd.to_datetime(dfo['endtime'],dayfirst=True)\n",
    "\n",
    "print('Adjust times done.\\n')\n",
    "\n",
    "print('Cell Completed!\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select desired response rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set desired response rates\n",
    "coverage_min = 1.0\n",
    "coverage_max = 100.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct data selection and get codebook information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get code book information done.\n",
      "\n",
      "Remove items based on coverage ...\n",
      "- compute coverage done.\n",
      "- removal done.\n",
      "- items with coverage between 1.0 % and 100.0 % will be removed.\n",
      "\n",
      "\n",
      "Remove non-numerical items ...\n",
      "\n",
      "Cell Completed!\n",
      "\n",
      "The remaining data is of size: (54505, 373)\n"
     ]
    }
   ],
   "source": [
    "# This is the dataframe we will use\n",
    "df = dfo.copy()\n",
    "\n",
    "filename = 'codebook_sb'\n",
    "ext = '.xlsx'\n",
    "\n",
    "df_codebook = pd.read_excel(datapath+filename+ext)\n",
    "\n",
    "item_dict = {}\n",
    "\n",
    "idx = df_codebook.index[df_codebook['col3'] == 'Value'].tolist()\n",
    "for ix in idx:\n",
    "    if df_codebook.iloc[ix-1,0].lower() in allcols:\n",
    "        item_dict.update( { df_codebook.iloc[ix-1,0].lower() : df_codebook.iloc[ix+1,2] } )\n",
    "        \n",
    "print('Get code book information done.\\n')\n",
    "\n",
    "\n",
    "print('Remove items based on coverage ...') \n",
    "item_coverage_dict = {}\n",
    "\n",
    "for col in df:\n",
    "    nans = df[col].isna()\n",
    "    coverage = 100 - 100*nans.mean()\n",
    "    #print(col,' : ',percentage,'%')\n",
    "    item_coverage_dict.update({ col:coverage})\n",
    "    \n",
    "print('- compute coverage done.')    \n",
    "\n",
    "deleted_cols = []\n",
    "\n",
    "#print('Will be deleted:')\n",
    "for col in df:\n",
    "    if df[col].dtype == 'float64':\n",
    "        coverage = item_coverage_dict[col]\n",
    "        if ((coverage < coverage_min) or (coverage > coverage_max)):\n",
    "            df = df.drop(columns=[col])\n",
    "            deleted_cols.append(col)\n",
    "            #print(col)\n",
    "\n",
    "print('- removal done.')            \n",
    "print('- items with coverage between',coverage_min,'% and',coverage_max,'% will be removed.\\n\\n')\n",
    "\n",
    "print('Remove non-numerical items ...')\n",
    "keep = ['endtime','gender','employment_status','state','disability','household_children','household_size', 'country' ]\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        if col not in keep:\n",
    "            #print(col)\n",
    "            df = df.drop(columns=[col])\n",
    "            deleted_cols.append(col)\n",
    "\n",
    "remaining_cols = list(df.columns)\n",
    "\n",
    "print('\\nCell Completed!\\n')\n",
    "print('The remaining data is of size:',df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the variables we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   recordno             endtime  qweek  i1_health  i2_health  i7a_health  \\\n",
      "0       0.0 2020-04-02 15:02:00    1.0        2.0        1.0         1.0   \n",
      "1       1.0 2020-04-02 15:02:00    1.0        0.0        0.0         1.0   \n",
      "2       2.0 2020-04-02 15:02:00    1.0        2.0        0.0         2.0   \n",
      "3       3.0 2020-04-02 15:02:00    1.0        3.0        0.0         0.0   \n",
      "4       4.0 2020-04-02 15:02:00    1.0        2.0        0.0         0.0   \n",
      "\n",
      "   i3_health  i4_health  i5_health_1  i5_health_2  ...  vac_man_99  had_covid  \\\n",
      "0        NaN        NaN          NaN          NaN  ...         NaN        NaN   \n",
      "1        4.0        4.0          0.0          0.0  ...         NaN        NaN   \n",
      "2        4.0        4.0          0.0          0.0  ...         NaN        NaN   \n",
      "3        4.0        5.0          0.0          0.0  ...         NaN        NaN   \n",
      "4        4.0        4.0          0.0          0.0  ...         NaN        NaN   \n",
      "\n",
      "   vac_boost_beyond  future_1  future_2  had_covid_2  long_covid  future_3  \\\n",
      "0               NaN       NaN       NaN          NaN         NaN       NaN   \n",
      "1               NaN       NaN       NaN          NaN         NaN       NaN   \n",
      "2               NaN       NaN       NaN          NaN         NaN       NaN   \n",
      "3               NaN       NaN       NaN          NaN         NaN       NaN   \n",
      "4               NaN       NaN       NaN          NaN         NaN       NaN   \n",
      "\n",
      "   country  case_id  \n",
      "0  germany      1.0  \n",
      "1  germany      2.0  \n",
      "2  germany      3.0  \n",
      "3  germany      4.0  \n",
      "4  germany      5.0  \n",
      "\n",
      "[5 rows x 373 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "66.58838638657005"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.head(10)\n",
    "#item_dict\n",
    "item_coverage_dict['i13_health']\n",
    "#remaining_cols\n",
    "#print(deleted_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Edits by Benjamin:\n",
    "    - added dataframe parameter to no longer use global dataframe df\n",
    "    - added possibility to save plot. To do so specify a valid directory as 'savedir'. Default behavior is to only show the plot in jupyter.\n",
    "'''\n",
    "def myerrorplot(df, item, savedir=None):\n",
    "    fig,axs = plt.subplots(nrows=1, ncols=2,gridspec_kw={'width_ratios': [5, 1]},figsize=(16, 3))\n",
    "    titlestring = item_dict[item]+' ('+item+')'\n",
    "    fig.suptitle(titlestring, fontsize=16)\n",
    "\n",
    "    # This works:\n",
    "    # df.groupby(df[\"endtime\"].dt.to_period(\"M\"))[item].mean().plot(kind='bar',ax=axs[0])\n",
    "\n",
    "    # But here with error bars\n",
    "    qual = df.groupby(df[\"endtime\"].dt.to_period(\"M\"))[item].agg([np.mean, np.std])\n",
    "    qual.plot(kind = \"bar\", y = \"mean\", legend = False, yerr = \"std\", color='green',ax=axs[0])\n",
    "    axs = df[item].hist(orientation=\"horizontal\",color='green',ax=axs[1])           # plot saved to variable to avoid automatic show by jupyter (now handled by plt.show() call below)\n",
    "    if savedir is not None:\n",
    "        plt.savefig(savedir + '/' + item + '.png', format='png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "'''\n",
    "Edits by Benjamin:\n",
    "    - added dataframe parameter to no longer use global dataframe df\n",
    "    - added possibility to save plot. To do so specify a valid directory as 'savedir'. Default behavior is to only show the plot in jupyter.\n",
    "'''\n",
    "def mybarplot(df, item, savedir=None):\n",
    "    fig,axs = plt.subplots(nrows=1, ncols=2,gridspec_kw={'width_ratios': [5, 1]},figsize=(16, 8))\n",
    "    titlestring = item_dict[item]+' ('+item+')'\n",
    "    fig.suptitle(titlestring, fontsize=16)\n",
    "    df.groupby(df[\"endtime\"].dt.to_period(\"M\"))[item].mean().plot(kind='bar',ax=axs[0])\n",
    "    axs = df[item].hist(orientation=\"horizontal\",ax=axs[1])                         # plot saved to variable to avoid automatic show by jupyter (now handled by plt.show() call below)\n",
    "    if savedir is not None:\n",
    "        plt.savefig(savedir + '/' + item + '.png', format='png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "def myhistplot(df, item):\n",
    "    fig,ax = plt.subplots(nrows=1, ncols=1,figsize=(8, 3))\n",
    "    titlestring = item_dict[item]+' ('+item+')'\n",
    "    fig.suptitle(titlestring, fontsize=16)\n",
    "    df[item].hist(orientation=\"vertical\",ax=ax)\n",
    "\n",
    "'''\n",
    "Edits by Benjamin:\n",
    "    - added dataframe parameter to no longer use global dataframe df\n",
    "    - added possibility to save plot. To do so specify a valid directory as 'savedir'. Default behavior is to only show the plot in jupyter.\n",
    "    - added possibility to specify correlation used. Parameter options: {'pearson', 'kendall', 'spearman'} or callable. Default: 'pearson'\n",
    "'''\n",
    "def correvoplot(df, items, periods, savedir=None, correlation='pearson'):\n",
    "    #periods = ['2020-04','2020-10','2021-04','2021-10','2022-04']\n",
    "    correlations = []\n",
    "    old_period = '2020-04'\n",
    "    for px,period in enumerate(periods[1:len(periods)]):\n",
    "        #print(period)\n",
    "        correlations.append( df.loc[(df[\"endtime\"] <= period) & (df[\"endtime\"] > old_period),items].corr(method=correlation).iloc[0][1])\n",
    "        old_period = period\n",
    "    #correlations\n",
    "\n",
    "    x = periods[1:len(periods)]\n",
    "    y = correlations\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    plt.xlabel(\"period\")\n",
    "    plt.ylabel(\"correlation\")\n",
    "    #plt.title(\"correlation over periods (\"+item_dict[items[0]]+' x '+item_dict[items[0]]+')')\n",
    "    title = \"correlation over periods (\"+items[0]+' x '+items[1]+')'\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.plot(x,y)\n",
    "\n",
    "    #plt.figsize(20,10)\n",
    "    if savedir is not None:\n",
    "        filename = savedir + '/' + title + '.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "'''\n",
    "Edits by Benjamin:\n",
    "    - added dataframe parameter to no longer use global dataframe df\n",
    "    - added possibility to save plot. To do so specify a valid directory as 'savedir'. Default behavior is to only show the plot in jupyter.\n",
    "    - added possibility to specify correlation used. Parameter options: {'pearson', 'kendall', 'spearman'} or callable. Default: 'pearson'\n",
    "'''\n",
    "def corrallevoplot(df, item, otheritems, periods, savedir=None, correlation='pearson'):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    plt.xlabel(\"period\")\n",
    "    plt.ylabel(\"correlation\")\n",
    "    title = \"correlation over periods\" + \" \" + str(item) +  \" with \" + str(otheritems)\n",
    "    plt.title(title)\n",
    "    for other in otheritems:\n",
    "        if item != other:\n",
    "            #print(other)\n",
    "            correlations = []\n",
    "            old_period = '2020-04'\n",
    "            for px,period in enumerate(periods[1:len(periods)]):\n",
    "                #print(period)\n",
    "                correlations.append( df.loc[ (df[\"endtime\"] <= period) & (df[\"endtime\"] > old_period),[item,other] ].corr(method=correlation).iloc[0][1])\n",
    "                old_period = period\n",
    "                #correlations\n",
    "\n",
    "            x = periods[1:len(periods)]\n",
    "            y = correlations\n",
    "            plt.plot(x,y,label='('+item+' x '+other+')')\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    #plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),ncol=4)\n",
    "    if savedir is not None:\n",
    "        filename = savedir + '/' + title + '.png'\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some simple period tests:\n",
      "['2020-11', '2020-12', '2021-01', '2021-02', '2021-03', '2021-04', '2021-05', '2021-06', '2021-07', '2021-08', '2021-09', '2021-10', '2021-11', '2021-12', '2022-01', '2022-02']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "''' Calculate a continuous list of time periods (months) format used in the dataframe ('YYYY-MM') from start to end (included).\n",
    " Parameters:\n",
    "   - start: str    starting timestring 'YYYY-MM'\n",
    "   - end: str      ending timestring   'YYYY-MM' (included)\n",
    "'''\n",
    "def calc_continous_periods(start:str, end:str):\n",
    "    periods = [start]\n",
    "    current = start\n",
    "    year, month = current.split('-')\n",
    "    end_year, end_month = end.split('-')\n",
    "    year, end_year = int(year), int(end_year)\n",
    "    month, end_month = int(month), int(end_month)\n",
    "\n",
    "    while not(year == end_year and month == end_month):\n",
    "        if month < 12:\n",
    "            month += 1\n",
    "        else:\n",
    "            month = 1\n",
    "            year += 1\n",
    "\n",
    "        year_str = str(year)\n",
    "        month_str = str(month)\n",
    "\n",
    "        if len(month_str) == 1:\n",
    "            month_str = '0' + month_str\n",
    "\n",
    "        periods.append(year_str + '-' + month_str)\n",
    "    return periods\n",
    "\n",
    "'''\n",
    "Checks save_dir directory for present plots, compares with desired plots in plots_todo and generates missing plots.\n",
    "Note by Benjamin: For some reason it did not calculate always calculate all the plots. Hence this function as workaround.\n",
    "'''\n",
    "def calc_missing_plots(plots_todo: list[str], save_dir: str, df: pd.DataFrame):\n",
    "    print(f\"Checking {save_dir} for missing plots...\")\n",
    "    existent_plots = os.listdir(save_dir)                           # get already existent plots\n",
    "    existent_labels = [item[:-4] for item in existent_plots]         # remove '.png' file ending\n",
    "    print(f\"Found {len(existent_labels)} plots. Required: {len(plots_todo)}\")\n",
    "\n",
    "    # sort lists for runtime optimization of naive checks below\n",
    "    existent_labels.sort()\n",
    "    plots_todo.sort()\n",
    "\n",
    "    for label in plots_todo:\n",
    "        if label not in existent_labels:\n",
    "            print(f\"Missing plot identified: {label}\")\n",
    "            mybarplot(df, label, save_dir)\n",
    "            print(f\"{label} plotted.\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "# simple test case for time period generation\n",
    "print(\"Some simple period tests:\")\n",
    "print(calc_continous_periods(start='2020-11',end='2022-02'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# At this point we are ready to start!\n",
    "\n",
    "### Example: Behavioral Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 labels identified. Trying to generate that many plots...\n",
      "Checking ['vac7', 'ox5_1', 'ox5_2', 'vac', 'm9_7', 'phq4_2', 'phq4_3', 'phq4_4', 'vac_man_99', 'vac12_booster_9', 'vac12_booster_8', 'vac12_booster_7', 'vac12_booster_6', 'vac12_booster_5', 'vac_boost_2', 'r1_8', 'vac_boost_1', 'r1_9', 'v4_98', 'av1_1', 'av1_2', 'vent_2', 'combi_vac', 'childvac', 'vac12_13', 'vac12_12', 'vac_booster', 'vac12_9', 'wcrex1', 'wcrex2', 'vac12_7', 'vac12_6', 'vac12_5', 'vac11', 'vac9', 'av3', 'av2', 'ct4', 'vac6', 'vac5', 'vac4', 'vac_3', 'vac2_5', 'vac2_4', 'vac2_3', 'vac_2', 'vac_1', 'ct6_3', 'ct6_4', 'ox6', 'ox3_6'] for missing plots...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "listdir: path should be string, bytes, os.PathLike, integer or None, not list",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 28\u001B[0m\n\u001B[1;32m     25\u001B[0m     file\u001B[38;5;241m.\u001B[39mwritelines(coverage_lines)\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# workaround for matplotlib sometimes not generating all plots\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m \u001B[43mcalc_missing_plots\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrustworthy_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mplots/coverage_plots\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 38\u001B[0m, in \u001B[0;36mcalc_missing_plots\u001B[0;34m(plots_todo, save_dir, df)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalc_missing_plots\u001B[39m(plots_todo: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m], save_dir: \u001B[38;5;28mstr\u001B[39m, df: pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChecking \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for missing plots...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 38\u001B[0m     existent_plots \u001B[38;5;241m=\u001B[39m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43msave_dir\u001B[49m\u001B[43m)\u001B[49m                           \u001B[38;5;66;03m# get already existent plots\u001B[39;00m\n\u001B[1;32m     39\u001B[0m     existent_labels \u001B[38;5;241m=\u001B[39m [item[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m4\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m existent_plots]         \u001B[38;5;66;03m# remove '.png' file ending\u001B[39;00m\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(existent_labels)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m plots. Required: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(plots_todo)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: listdir: path should be string, bytes, os.PathLike, integer or None, not list"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Code by Benjamin to plot coverage_plots (barplots) and file with coverage_data\n",
    "'''\n",
    "# labels of questions deemed relevant for trust analysis (or 'worthy of', hence called 'trustworthy')\n",
    "# extracted by the other team members carefully crawling over the entire codebook questions and seaching for trust related questions with data coverage above 1%\n",
    "trustworthy_labels = ['vac7', 'ox5_1', 'ox5_2', 'vac', 'm9_7', 'phq4_2', 'phq4_3', 'phq4_4', 'vac_man_99', 'vac12_booster_9', 'vac12_booster_8', 'vac12_booster_7', 'vac12_booster_6', 'vac12_booster_5', 'vac_boost_2', 'r1_8', 'vac_boost_1', 'r1_9', 'V4_98', 'av1_1', 'av1_2', 'vent_2', 'combi_vac', 'childvac', 'vac12_13', 'vac12_12', 'vac_booster', 'vac12_9', 'wcrex1', 'wcrex2', 'vac12_7', 'vac12_6', 'vac12_5', 'vac11', 'vac9', 'av3', 'av2', 'ct4', 'vac6', 'vac5', 'vac4', 'vac_3', 'vac2_5', 'vac2_4', 'vac2_3', 'vac_2', 'vac_1', 'ct6_3', 'ct6_4', 'ox6', 'ox3_6']\n",
    "\n",
    "trustworthy_labels = [label.lower() for label in trustworthy_labels]\n",
    "print(f\"{len(trustworthy_labels)} labels identified. Trying to generate that many plots...\")\n",
    "\n",
    "# check whether labels exist in df\n",
    "df_labels = df.keys()\n",
    "for label in trustworthy_labels:\n",
    "    if label not in df_labels:\n",
    "        print(f\"WARNING: label {label} NOT IN DF!!\")\n",
    "\n",
    "# plot all coverage_plots to specified directory\n",
    "coverage_lines = []\n",
    "for label in trustworthy_labels:\n",
    "    coverage_lines.append(label + ' ' + str(item_coverage_dict[label])+ '\\n')\n",
    "    mybarplot(df, label, savedir='plots/coverage_plots')\n",
    "\n",
    "# print coverage for each trustworthy_label to a .txt file\n",
    "with open('coverage_data.txt', 'w') as file:\n",
    "    file.writelines(coverage_lines)\n",
    "\n",
    "# workaround for matplotlib sometimes not generating all plots\n",
    "calc_missing_plots(df, trustworthy_labels, 'plots/coverage_plots')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Code by Benjamin used to plot correlation plots\n",
    "'''\n",
    "\n",
    "# first filter dataframe\n",
    "df['wcrex1'] = df['wcrex1'].replace(5.0, np.nan)\n",
    "df['wcrex2'] = df['wcrex2'].replace(5.0, np.nan)\n",
    "\n",
    "cor_df = df.copy()\n",
    "\n",
    "cor_df['vac7'] = cor_df['vac7'] * (-1) + 5\n",
    "\n",
    "\n",
    "\n",
    "periods = calc_continous_periods('2011-01', '2032-03')\n",
    "vac = 'vac7'\n",
    "gov = 'wcrex1'\n",
    "nhs = 'wcrex2'\n",
    "\n",
    "correlation_independents = [(vac, gov), (vac, nhs), (gov, nhs)]\n",
    "\n",
    "correlate_with_gov = ['vac12_12', 'vac12_13']\n",
    "correlate_with_nhs = ['vac12_5', 'vac12_booster_5', 'vac12_booster_6', 'vac12_booster_7']\n",
    "correlate_with_vac = ['vac_booster', 'vac12_booster_5', 'vac12_booster_6', 'vac12_booster_7']\n",
    "\n",
    "for corr_items in correlation_independents:\n",
    "    correvoplot(cor_df, corr_items, periods, correlation='spearman', savedir='plots/correlation_plots')\n",
    "\n",
    "corrallevoplot(cor_df, gov, correlate_with_gov, periods, correlation='spearman', savedir='plots/correlation_plots')\n",
    "corrallevoplot(cor_df, nhs, correlate_with_nhs, periods, correlation='spearman', savedir='plots/correlation_plots')\n",
    "corrallevoplot(cor_df, vac, correlate_with_vac, periods, correlation='spearman', savedir='plots/correlation_plots')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mybarplot('vac5')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "myerrorplot('vac7')\n",
    "myerrorplot('ox5_1')\n",
    "myerrorplot('ox5_2')\n",
    "myerrorplot('wcrex2')\n",
    "myerrorplot('wcrex1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "items_behavior = [ i for i in remaining_cols if 'i12_health' in i]\n",
    "#items_behavior.extend(['i13_health'])\n",
    "\n",
    "print(items_behavior)\n",
    "\n",
    "for i in items_behavior:\n",
    "    #mybarplot(i)\n",
    "    myerrorplot(i)\n",
    "    #myhistplot(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
